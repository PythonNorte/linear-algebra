\section{Notes for teaching}

The ``Notes for teaching'' section in each chapter includes some brief notes from instructors that have used the book before.  These comments are not edited extensively, but rather are more free-form reflective thoughts about the chapter and how to teach it.

This might make sense to move to a wiki eventually, maybe the wiki on bitbucket or github (or wherever the book text is hosted).

This chapter is a brief overview of some of the arithmetic we'll be using throughout the course.  It's easy to get bogged down in the theory---much of that discussion will come later in the course.

\subsection{Ben Woodruff}

I've been thinking a lot about why I get through chapter 1 so fast.  I think a lot of it has to do with me skipping many introductory examples. I just assign them to do the first few problems and don't ever really lecture on them at all. I start Gaussian elimination on Day 2, and then by Day 3 I'm through with rank, span, and independence (really we just spend the day practicing more with Gaussian elimination and then adding a few comments).  Near the end of day 3, I'll talk about determinants, and if there is time, connect them to area and volume, and have them explain to me what is special about a zero determinant (on a 2 by 2 example). They can see the vectors are dependent.  Day 4 we spend on determinants and inverses (and catch up if needed with repeated questions about rank, dependence, etc. because once they row reduce a matrix there are lots of quick question I can ask).  Day 5 I start with making a connection between inverses, determinants, and dependence (if not already done on day 4), and then end with some eigenvalue/vector stuff.  The last day I would spend on eigenvalues/vectors.

I think one key is to not lecture on matrix and vector arithmetic, but instead let them do it for homework.  I didn't spend much time with the dot product and why a zero dot product means a 90 degree angle.  I let them read those facts.  The hardest topic to get students to see is the concept of linearly independent and dependent.  I think the definition ``any linearly combination which equals zero is the zero combination'' is not the best definition for gaining understanding (though it is the best definition for proving theorems).  I always introduce dependent first by saying, ``one vector is a linear combination of the others, so that vector depends on the others,'' and I show them how to find the dependence immediately from rref. Since they just finished row reducing the example I am about to use, it makes the class practice time lead right into the lecture.  This is a definition of dependence students can grab onto and see immediately. I'll talk about force vectors and a machine that can push or pull things in only 2 ways (hence can get a whole plane of pushes and pulls by combining things - which is where I introduce the word ``span'').  They see that vectors are dependent when one column does not have a pivot and independent if every column has a pivot. Span comes out of this discussion for free---it's a side comment while I'm discussing dependence (though I'll follow it up with some 3d examples and pieces of paper to represent planes). I won't go back and revise their understanding of independence as the first definition until chapter~3.

I think I could do better in the eigenvector section by reusing the word ``span'' more.  The problem is that I don't want to get into ``bases'' yet, though I could easily use the word ``span'' more without causing problems. However, then we have to deal with excluding the zero vector from the span. This would emphasize more the notion of a ``vector space'' prior to them seeing the formal definition.
